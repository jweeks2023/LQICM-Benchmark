# LQICM-Benchmark
A repo containing the benchmark program used in the LQICM senior capstone project.

## Contents
- README.md: This File!
- benchmark.c: The benchmarking code in question.
- CodeToTest Folder: This is where you put your code to test
- Outputs Folder: This is where the results of the benchmark are output

## Prerequisites
- A computer/virtual machine running Linux
    - Note: Further testing needed on other operating systems before I can confirm that it's functional on Windows/Mac
- A C compiler (I recommend LLVM or GCC)
- Code that you want to benchmark
- A general understanding of how to use a terminal and C compilers like LLVM, Clang, or GCC

## How To Use:
### Set Up
1. Download/clone the repo to the desired location.
2. In your file explorer, open the `LQICM-Benchmark` folder to make sure all the files from the Contents section are there.
3. If desired, open the `benchmark.c` file in a text editor (I recommend Notepad++) and change the parameters at the top of the program.
    - `ITERATIONS` - Number of times each file is benchmarked. (default: 20)
    - `NEATDISPLAY` - Determines whether the benchmark data is output in a table. true = table, false = no table. (default: true)
    - `COMPILER` - Specifies what C compiler you're using. (default: "clang-15")
    - `COMPILERPATH` - Specifies the filepath for the compiler above. (default: "../../../usr/bin/")
    - `INPUTFOLDER` - Specifies the filepath for the folder that will contain the C files that you want to benchmark (default: "./CodeToTest/")
    - `OUTPUTFOLDER` - Specifies the filepath for the folder that will contain all the output files that is generated by the benchmark (default: "./Outputs/")
    - `OPTLEVEL` - Specifies what optimization level your compiler will apply to the files being benchmarked (default: O0)
        - For more information on optimization levels, see [this link](https://clang.llvm.org/docs/CommandGuide/clang.html#cmdoption-o0)
4. Save `benchmark.c` if any changes were made.
5. If not already there, naviagte to the `LQICM-Benchmark` folder in the terminal window.
6. Compile `benchmark.c`.
    - Note: We recommend using clang since it comes with LLVM and is more efficient than gcc. Ultimately, you can use any C compiler you like.
    - Example using clang-15: `clang-15 benchmark.c -o benchmark -lm`.
### Inserting Code
1. In your file explorer, navigate to the `CodeToTest` folder.
2. Insert your own test code as .c files in the folder. Each C file will be timed separately, and will output in separate output files.
    - Note: Make sure your code compiles and runs as expected before attempting to benchmark it. Failure to do so may cause inaccurate benchmark results. An example file is provided in the `CodeToTest` folder.
### Running the Benchmark
1. Make sure you're in the `Benchmark` folder and run the benchmark using the following command: `./benchmark`.
2. See output file in the `Outputs` folder for results.
### Reviewing Results
- Output files will be in the format `output YYYY-MM-DD HHMMSS.txt`. The title of the C program that is timed is contained at the top of the file.
- Data will be in seconds, with 6 decimals of precision.

## Common Bugs
`sh: 1: [OBJECT OR FILE PATH]: not found` - Most likely the compilation of the C file being benchmark failed. Check the `COMPILERPATH` parameter to make sure it points to the same folder `COMPILER` exists in.

Any error message that displays repeatedly in rapid succession - This is most likely an error with your C file. Make sure you can compile and run your code through your compiler before putting it in the `CodeToTest` folder.

`Command '[COMPILER]' not found, but can be installed with:` - This means that you are referencing a compiler that you do not have installed. This issue is common when you install LLVM, as many commands require the command followed by `-[VERSION NUMBER]`. This is why the default value for `COMPILER` is `clang-15`. Please verify you've installed a C compiler and that you are referencing the correct version in the command.

Created by Jason Weeks, Justice Howley, and Ian Yelle, 2023 
